{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/readout/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import json\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "\n",
    "from readout_training import train_helpers\n",
    "from readout_training.train_spatial import get_spatial_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PascalVOC annotations\n",
    "pascalvoc_root = \"data/raw/PascalVOC/VOC2012/JPEGImages/*\"\n",
    "pascalvoc_anns = train_helpers.create_image_anns(pascalvoc_root, \"PascalVOC\")\n",
    "\n",
    "# Split into train / val\n",
    "num_val = min(int(0.1 * len(pascalvoc_anns)), 1000)\n",
    "np.random.seed(0)\n",
    "idxs = np.random.permutation(range(len(pascalvoc_anns)))\n",
    "pascalvoc_val_idxs, pascalvoc_train_idxs = set(idxs[:num_val]), set(idxs[num_val:])\n",
    "pascalvoc_train = [ann for i, ann in enumerate(pascalvoc_anns) if i in pascalvoc_train_idxs]\n",
    "pascalvoc_val = [ann for i, ann in enumerate(pascalvoc_anns) if i in pascalvoc_val_idxs]\n",
    "assert len(pascalvoc_train) + len(pascalvoc_val) == len(pascalvoc_anns)\n",
    "\n",
    "# Save annotations\n",
    "json.dump(pascalvoc_train, open(\"annotations/PascalVOC_train.json\", \"w\"))\n",
    "json.dump(pascalvoc_val, open(\"annotations/PascalVOC_val.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DAVIS annotations\n",
    "davis_root = \"data/raw/DAVIS/JPEGImages/480p/*\"\n",
    "davis_anns = train_helpers.create_video_anns(davis_root, \"DAVIS\")\n",
    "\n",
    "# Split into train / val\n",
    "open_split = lambda split: set(open(f\"data/raw/DAVIS/ImageSets/2017/{split}.txt\", \"r\").read().split(\"\\n\"))\n",
    "davis_train_names = open_split(\"train\")\n",
    "davis_val_names = open_split(\"val\")\n",
    "\n",
    "davis_train = [ann for ann in davis_anns if ann[\"video_name\"] in davis_train_names]\n",
    "davis_val = [ann for ann in davis_anns if ann[\"video_name\"] in davis_val_names]\n",
    "assert len(davis_train) + len(davis_val) == len(davis_anns)\n",
    "\n",
    "# Save annotations\n",
    "json.dump(davis_train, open(\"annotations/DAVIS_train.json\", \"w\"))\n",
    "json.dump(davis_val, open(\"annotations/DAVIS_val.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Pose Head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the Diffusion Extractor and Readout Head**\n",
    "\n",
    "The demo is pre-loaded to the SDXL pose readout head. To try other spatial heads, update `dataset_args` in the config and `aggregation_ckpt` in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "config_path = \"configs/train_spatial.yaml\"\n",
    "config = OmegaConf.load(config_path)\n",
    "aggregation_ckpt = \"../weights/readout_sdxl_spatial_pose.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06eb69c49ab047fd942ad93e96082f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config, diffusion_extractor, aggregation_network = train_helpers.load_models(config_path, device=device)\n",
    "state_dict = torch.load(aggregation_ckpt)\n",
    "aggregation_network.load_state_dict(state_dict[\"aggregation_network\"], strict=False)\n",
    "aggregation_network = aggregation_network.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract Readouts**\n",
    "\n",
    "Extract the readouts for a single validation batch of real images. Set `eval_mode`=True to extract a readout from the clean image; if this is set to False then the input image is noised according to a random timestep. We also plot the readout head's learned mixing weights, which visualizes the influence of the decoder layers (bright yellow = high weight, dark blue = low weight). Earlier low-resolution layers (1) tend to be more \"semantic\" and later high-resolution layers (9) tend to be more \"textural\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_mode = True\n",
    "val_dataset, val_dataloader = get_spatial_loader(config, config[\"val_file\"], False)\n",
    "for i, ann in enumerate(val_dataloader):\n",
    "    batch = ann\n",
    "    imgs, target = batch[\"source\"], batch[\"control\"]\n",
    "    pred = train_helpers.get_hyperfeats(diffusion_extractor, aggregation_network, imgs.to(device), eval_mode=eval_mode)\n",
    "    target = train_helpers.standardize_feats(imgs, target)\n",
    "    pred = train_helpers.standardize_feats(imgs, pred)\n",
    "    grid = train_helpers.log_grid(imgs, target, pred, val_dataset.control_range)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_sep = \"=\" * 80\n",
    "print(prompt_sep)\n",
    "print(\"(Top) Input Image, (Middle) Target Pseudo Label, (Bottom) Predicted Readout\")\n",
    "print(prompt_sep)\n",
    "display(grid)\n",
    "print(prompt_sep)\n",
    "print(\"Aggregation Network Mixing Weights\")\n",
    "print(prompt_sep)\n",
    "fig = train_helpers.log_aggregation_network(aggregation_network, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "readout",
   "language": "python",
   "name": "readout"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
